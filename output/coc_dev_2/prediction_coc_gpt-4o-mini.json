[{"instance_id": "sqlfluff__sqlfluff-1625", "model_patch": "diff --git a/src/sqlfluff/rules/L031.py b/src/sqlfluff/rules/L031.py\nindex 838c6ce5d..973cd1818 100644\n--- a/src/sqlfluff/rules/L031.py\n+++ b/src/sqlfluff/rules/L031.py\n@@ -150,8 +150,10 @@ class Rule_L031(BaseRule):\n         self, base_table, from_expression_elements, column_reference_segments, segment\n     ):\n         \"\"\"Lint and fix all aliases in joins - except for self-joins.\"\"\"\n-        # A buffer to keep any violations.\n-        violation_buff = []\n+        \n+        # New Check for actual join conditions\n+        if not any('JOIN' in clause.raw for clause in from_expression_elements):\n+            return None  # No joins present, skip linting for aliases\n \n         to_check = list(\n             self._filter_table_expressions(base_table, from_expression_elements)\n", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "sqlfluff__sqlfluff-2419", "model_patch": "diff --git a/src/sqlfluff/rules/L060.py b/src/sqlfluff/rules/L060.py\nindex 836941edc..bce6db8e0 100644\n--- a/src/sqlfluff/rules/L060.py\n+++ b/src/sqlfluff/rules/L060.py\n@@ -43,20 +43,38 @@ class Rule_L060(BaseRule):\n         if context.segment.name != \"function_name_identifier\":\n             return None\n \n-        # Only care if the function is ``IFNULL`` or ``NVL``.\n-        if context.segment.raw_upper not in {\"IFNULL\", \"NVL\"}:\n-            return None\n-\n-        # Create fix to replace ``IFNULL`` or ``NVL`` with ``COALESCE``.\n-        fix = LintFix.replace(\n-            context.segment,\n-            [\n-                CodeSegment(\n-                    raw=\"COALESCE\",\n-                    name=\"function_name_identifier\",\n-                    type=\"function_name_identifier\",\n-                )\n-            ],\n-        )\n+        if context.segment.raw_upper == \"IFNULL\":\n+            # Create fix to replace ``IFNULL`` with ``COALESCE``.\n+            fix = LintFix.replace(\n+\n+                context.segment,\n+                [\n+                    CodeSegment(\n+                        raw=\"COALESCE\",\n+                        name=\"function_name_identifier\",\n+                        type=\"function_name_identifier\",\n+                    )\n+                ],\n+            )\n+            return LintResult(context.segment, [fix], \n+                              f\"Use 'COALESCE' instead of 'IFNULL'.\")\n+\n+        if context.segment.raw_upper == \"NVL\":\n+            # Create fix to replace ``NVL`` with ``COALESCE``.\n+            fix = LintFix.replace(\n+\n+                context.segment,\n+                [\n+                    CodeSegment(\n+                        raw=\"COALESCE\",\n+                        name=\"function_name_identifier\",\n+                        type=\"function_name_identifier\",\n+                    )\n+                ],\n+            )\n+            return LintResult(context.segment, [fix], \n+                              f\"Use 'COALESCE' instead of 'NVL'.\")\n+\n+        return None\n \n         return LintResult(context.segment, [fix])\n", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "sqlfluff__sqlfluff-1733", "model_patch": "diff --git a/src/sqlfluff/core/parser/segments/base.py b/src/sqlfluff/core/parser/segments/base.py\nindex a95a07632..90dfb9e71 100644\n--- a/src/sqlfluff/core/parser/segments/base.py\n+++ b/src/sqlfluff/core/parser/segments/base.py\n@@ -915,7 +915,10 @@ class BaseSegment:\n                                     seg_buffer.append(f.edit)  # pragma: no cover TODO?\n                                 else:\n                                     for s in f.edit:\n-                                        seg_buffer.append(s)\n+                                        # Handle leading whitespace correctly\n+                                        if s.raw.strip() and s.raw.lstrip() not in seen_segments:\n+                                            seen_segments.add(s.raw.lstrip())\n+                                            seg_buffer.append(s.raw.lstrip())\n \n                                 if f.edit_type == \"create\":\n                                     # in the case of a creation, also add this segment on the end\n", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "sqlfluff__sqlfluff-1517", "model_patch": "diff --git a/src/sqlfluff/core/parser/parsers.py b/src/sqlfluff/core/parser/parsers.py\nindex 519a4c888..16cc8c9c9 100644\n--- a/src/sqlfluff/core/parser/parsers.py\n+++ b/src/sqlfluff/core/parser/parsers.py\n@@ -51,13 +51,13 @@ class StringParser(Matchable):\n         \"\"\"\n         return [self.template]\n \n-    def _is_first_match(self, segment: BaseSegment):\n-        \"\"\"Does the segment provided match according to the current rules.\"\"\"\n-        # Is the target a match and IS IT CODE.\n-        # The latter stops us accidentally matching comments.\n+        # Check for valid SQL matching and ensure we can handle extra semicolons.\n         if self.template == segment.raw.upper() and segment.is_code:\n             return True\n-        return False\n+        # Allow multiple semicolons at the end of the SQL statement\n+        elif segment.raw.upper().endswith(';;') and segment.is_code:\n+            return True\n+\n \n     def _make_match_from_first_result(self, segments: Tuple[BaseSegment, ...]):\n         \"\"\"Make a MatchResult from the first segment in the given list.\n", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "sqlfluff__sqlfluff-1763", "model_patch": "diff --git a/src/sqlfluff/core/templaters/jinja.py b/src/sqlfluff/core/templaters/jinja.py\nindex 38d2c41eb..80068e7ba 100644\n--- a/src/sqlfluff/core/templaters/jinja.py\n+++ b/src/sqlfluff/core/templaters/jinja.py\n@@ -60,8 +60,9 @@ class JinjaTemplater(PythonTemplater):\n         macro_ctx = {}\n         if os.path.isfile(path):\n             # It's a file. Extract macros from it.\n-            with open(path) as opened_file:\n-                template = opened_file.read()\n+            with open(path, encoding='utf-8') as opened_file:\n+                template = opened_file.read()  # Read the file in UTF-8 encoding.\n+\n             # Update the context with macros from the file.\n             macro_ctx.update(\n                 cls._extract_macros_from_template(template, env=env, ctx=ctx)\n@@ -289,26 +290,23 @@ class JinjaTemplater(PythonTemplater):\n                 ),\n                 violations,\n             )\n-        except (TemplateError, TypeError) as err:\n-            templater_logger.info(\"Unrecoverable Jinja Error: %s\", err)\n-            violations.append(\n-                SQLTemplaterError(\n-                    (\n-                        \"Unrecoverable failure in Jinja templating: {}. Have you configured \"\n-                        \"your variables? https://docs.sqlfluff.com/en/latest/configuration.html\"\n-                    ).format(err)\n-                )\n+            out_str = template.render()\n+            # Slice the file once rendered.\n+            raw_sliced, sliced_file, out_str = self.slice_file(\n+                in_str, out_str, config=config\n+            )\n+            # Ensure output is written with utf-8 encoding.\n+            return (\n+                TemplatedFile(\n+                    source_str=in_str,\n+                    templated_str=out_str,\n+                    fname=fname,\n+                    sliced_file=sliced_file,\n+                    raw_sliced=raw_sliced,\n+                ),\n+                violations,\n             )\n-            return None, violations\n-\n-    re_open_tag = re.compile(r\"^\\s*{%[\\+\\-]?\\s*\")\n-    re_close_tag = re.compile(r\"\\s*[\\+\\-]?%}\\s*$\")\n-\n-    @classmethod\n-    def _slice_template(cls, in_str: str) -> Iterator[RawFileSlice]:\n-        \"\"\"Slice template in jinja.\n \n-        NB: Starts and ends of blocks are not distinguished.\n         \"\"\"\n         env = cls._get_jinja_env()\n         str_buff = \"\"\n", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "marshmallow-code__marshmallow-1359", "model_patch": "diff --git a/src/marshmallow/fields.py b/src/marshmallow/fields.py\nindex 0b18e7dd..8942833c 100644\n--- a/src/marshmallow/fields.py\n+++ b/src/marshmallow/fields.py\n@@ -1114,9 +1114,9 @@ class DateTime(Field):\n         super()._bind_to_schema(field_name, schema)\n         self.format = (\n             self.format\n-            or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)\n+            or getattr(schema, 'opts', {}).get(self.SCHEMA_OPTS_VAR_NAME)\n             or self.DEFAULT_FORMAT\n-        )\n+\n \n     def _serialize(self, value, attr, obj, **kwargs):\n         if value is None:\n", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "marshmallow-code__marshmallow-1343", "model_patch": "diff --git a/src/marshmallow/schema.py b/src/marshmallow/schema.py\nindex 085c509b..7d75ebe7 100644\n--- a/src/marshmallow/schema.py\n+++ b/src/marshmallow/schema.py\n@@ -610,8 +610,11 @@ class BaseSchema(base.SchemaABC):\n         data = self.opts.json_module.loads(json_data, *args, **kwargs)\n         return self.load(data, many=many, partial=partial)\n \n-    def validate(self, data, many=None, partial=None):\n-        \"\"\"Validate `data` against the schema, returning a dictionary of\n+    @validates('value')  # <- issue here\n+    def validate_value(self, value):\n+        if value is None:\n+            raise ValidationError(\"Value cannot be None.\")\n+\n         validation errors.\n \n         :param dict data: The data to validate.\n", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "pvlib__pvlib-python-1707", "model_patch": "diff --git a/pvlib/iam.py b/pvlib/iam.py\nindex 3eaa6b4..b2d3dc5 100644\n--- a/pvlib/iam.py\n+++ b/pvlib/iam.py\n@@ -91,7 +91,10 @@ def ashrae(aoi, b=0.05):\n     return iam\n \n \n-def physical(aoi, n=1.526, K=4.0, L=0.002, *, n_ar=None):\n+    # Early return for AOI >= 90\n+    if np.any(np.abs(aoi) >= 90.0):\n+        return 0.0\n+\n     r\"\"\"\n     Determine the incidence angle modifier using refractive index ``n``,\n     extinction coefficient ``K``, glazing thickness ``L`` and refractive\n", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "pvlib__pvlib-python-1072", "model_patch": "diff --git a/pvlib/temperature.py b/pvlib/temperature.py\nindex 1d98736..b96f915 100644\n--- a/pvlib/temperature.py\n+++ b/pvlib/temperature.py\n@@ -599,8 +599,8 @@ def fuentes(poa_global, temp_air, wind_speed, noct_installed, module_height=5,\n     # n.b. the way Fuentes calculates the first timedelta makes it seem like\n     # the value doesn't matter -- rather than recreate it here, just assume\n     # it's the same as the second timedelta:\n-    timedelta_hours = np.diff(poa_global.index).astype(float) / 1e9 / 60 / 60\n-    timedelta_hours = np.append([timedelta_hours[0]], timedelta_hours)\n+    timedelta_hours = np.diff(poa_global.index).astype('timedelta64[s]').astype(float) / 3600\n+\n \n     tamb_array = temp_air + 273.15\n     sun_array = poa_global * absorp\n", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "pvlib__pvlib-python-1606", "model_patch": "diff --git a/pvlib/tools.py b/pvlib/tools.py\nindex 991568f..77ff889 100644\n--- a/pvlib/tools.py\n+++ b/pvlib/tools.py\n@@ -299,9 +299,11 @@ def _build_args(keys, input_dict, dict_name):\n # Author: Rob Andrews, Calama Consulting\n # Modified: November, 2020 by C. W. Hansen, to add atol and change exit\n # criteria\n-def _golden_sect_DataFrame(params, lower, upper, func, atol=1e-8):\n-    \"\"\"\n-    Vectorized golden section search for finding maximum of a function of a\n+    # Check for equal bounds and handle gracefully\n+    if np.isclose(lower, upper):\n+        # Return NaN if upper and lower bounds are the same\n+        return np.nan, np.nan\n+\n     single variable.\n \n     Parameters\n", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "pvlib__pvlib-python-1854", "model_patch": "diff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\nindex c277a77..4783d85 100644\n--- a/pvlib/pvsystem.py\n+++ b/pvlib/pvsystem.py\n@@ -202,30 +202,9 @@ class PVSystem:\n                  inverter=None, inverter_parameters=None,\n                  racking_model=None, losses_parameters=None, name=None):\n \n-        if arrays is None:\n-            if losses_parameters is None:\n-                array_losses_parameters = {}\n-            else:\n-                array_losses_parameters = _build_kwargs(['dc_ohmic_percent'],\n-                                                        losses_parameters)\n-            self.arrays = (Array(\n-                FixedMount(surface_tilt, surface_azimuth, racking_model),\n-                albedo,\n-                surface_type,\n-                module,\n-                module_type,\n-                module_parameters,\n-                temperature_model_parameters,\n-                modules_per_string,\n-                strings_per_inverter,\n-                array_losses_parameters,\n-            ),)\n-        elif len(arrays) == 0:\n-            raise ValueError(\"PVSystem must have at least one Array. \"\n-                             \"If you want to create a PVSystem instance \"\n-                             \"with a single Array pass `arrays=None` and pass \"\n-                             \"values directly to PVSystem attributes, e.g., \"\n-                             \"`surface_tilt=30`\")\n+        elif isinstance(arrays, Array):\n+            self.arrays = (arrays,)  # Wrap a single Array in a tuple\n+\n         else:\n             self.arrays = tuple(arrays)\n \n", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "pvlib__pvlib-python-1154", "model_patch": "diff --git a/pvlib/irradiance.py b/pvlib/irradiance.py\nindex 6580030..6fa865f 100644\n--- a/pvlib/irradiance.py\n+++ b/pvlib/irradiance.py\n@@ -886,7 +886,12 @@ def reindl(surface_tilt, surface_azimuth, dhi, dni, ghi, dni_extra,\n     # these are the () and [] sub-terms of the second term of eqn 8\n     term1 = 1 - AI\n     term2 = 0.5 * (1 + tools.cosd(surface_tilt))\n-    term3 = 1 + np.sqrt(HB / ghi) * (tools.sind(0.5 * surface_tilt) ** 3)\n+        # Calculate term3 with a check for ghi being zero\n+        if ghi == 0:\n+            term3 = 1  # Assign 1 when ghi is zero\n+        else:\n+            term3 = 1 + np.sqrt(HB / ghi) * (tools.sind(0.5 * surface_tilt) ** 3)\n+\n \n     sky_diffuse = dhi * (AI * Rb + term1 * term2 * term3)\n     sky_diffuse = np.maximum(sky_diffuse, 0)\n", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "pylint-dev__astroid-1978", "model_patch": "", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "pylint-dev__astroid-1333", "model_patch": "", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "pylint-dev__astroid-1196", "model_patch": "diff --git a/astroid/nodes/node_classes.py b/astroid/nodes/node_classes.py\nindex 11136f8c..19f25d1f 100644\n--- a/astroid/nodes/node_classes.py\n+++ b/astroid/nodes/node_classes.py\n@@ -2362,8 +2362,8 @@ class Dict(NodeNG, Instance):\n             if isinstance(key, DictUnpack):\n                 try:\n                     return value.getitem(index, context)\n-                except (AstroidTypeError, AstroidIndexError):\n-                    continue\n+                    pass  # Skip to the next key, do not continue the exception chain\n+\n             for inferredkey in key.infer(context):\n                 if inferredkey is util.Uninferable:\n                     continue\n", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "pylint-dev__astroid-1866", "model_patch": "diff --git a/astroid/brain/brain_builtin_inference.py b/astroid/brain/brain_builtin_inference.py\nindex af1ddf4d..a9d0244c 100644\n--- a/astroid/brain/brain_builtin_inference.py\n+++ b/astroid/brain/brain_builtin_inference.py\n@@ -930,11 +930,14 @@ def _infer_str_format_call(\n     \"\"\"Return a Const node based on the template and passed arguments.\"\"\"\n     call = arguments.CallSite.from_call(node, context=context)\n     if isinstance(node.func.expr, nodes.Name):\n-        value: nodes.Const = helpers.safe_infer(node.func.expr)\n-    else:\n-        value = node.func.expr\n+\t        value = helpers.safe_infer(node.func.expr)\n+\t        if value is None:\n+\t            return iter([util.Uninferable])\n+\t    else:\n+\t        value = node.func.expr\n+\t        if value is None:\n+\t            return iter([util.Uninferable])\n \n-    format_template = value.value\n \n     # Get the positional arguments passed\n     inferred_positional = [\n", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "pylint-dev__astroid-1268", "model_patch": "diff --git a/astroid/nodes/as_string.py b/astroid/nodes/as_string.py\nindex 427ccc15..67d55155 100644\n--- a/astroid/nodes/as_string.py\n+++ b/astroid/nodes/as_string.py\n@@ -637,7 +637,10 @@ class AsStringVisitor:\n     def visit_uninferable(self, node):\n         return str(node)\n \n-    def visit_property(self, node):\n+    def visit_unknown(self, node):\n+        \"\"\"Handle an unknown node type.\"\"\"\n+        return \"Unknown node type\"\n+\n         return node.function.accept(self)\n \n     def visit_evaluatedobject(self, node):\n", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "pyvista__pyvista-4315", "model_patch": "diff --git a/pyvista/core/grid.py b/pyvista/core/grid.py\nindex 6ce23688..e9a5cb63 100644\n--- a/pyvista/core/grid.py\n+++ b/pyvista/core/grid.py\n@@ -127,33 +127,41 @@ class RectilinearGrid(_vtk.vtkRectilinearGrid, Grid, RectilinearGridFilters):\n         \"\"\"Initialize the rectilinear grid.\"\"\"\n         super().__init__()\n \n-        if len(args) == 1:\n-            if isinstance(args[0], _vtk.vtkRectilinearGrid):\n-                if deep:\n-                    self.deep_copy(args[0])\n-                else:\n-                    self.shallow_copy(args[0])\n-            elif isinstance(args[0], (str, pathlib.Path)):\n-                self._from_file(args[0], **kwargs)\n-            elif isinstance(args[0], np.ndarray):\n-                self._from_arrays(args[0], None, None, check_duplicates)\n-            else:\n-                raise TypeError(f'Type ({type(args[0])}) not understood by `RectilinearGrid`')\n+130\t        if len(args) == 1:\n+131\t            if isinstance(args[0], _vtk.vtkRectilinearGrid):\n+132\t                if deep:\n+133\t                    self.deep_copy(args[0])\n+134\t                else:\n+135\t                    self.shallow_copy(args[0])\n+136\t            elif isinstance(args[0], (str, pathlib.Path)):\n+137\t                self._from_file(args[0], **kwargs)\n+138\t            elif isinstance(args[0], np.ndarray):\n+139\t                self._from_arrays(args[0], None, None, check_duplicates)\n+140\t            # Allow input of Sequence types (like lists or tuples)\n+141\t            elif isinstance(args[0], Sequence) and not isinstance(args[0], np.ndarray):\n+141.1\t                args[0] = np.array(args[0])  # Convert Sequence to ndarray\n+141.2\t                self._from_arrays(args[0], None, None, check_duplicates)\n+141.3\t            else:\n+141.4\t                raise TypeError(f'Type ({type(args[0])}) not understood by `RectilinearGrid`')\n+\n+\n+143\t        elif len(args) == 3 or len(args) == 2:\n+144\t            arg0_is_arr = isinstance(args[0], np.ndarray)\n+145\t            arg1_is_arr = isinstance(args[1], np.ndarray)\n+146\t            if len(args) == 3:\n+147\t                arg2_is_arr = isinstance(args[2], np.ndarray)\n+148\t            else:\n+149\t                arg2_is_arr = False\n+150\t\n+151\t            # Check if arg0 is a Sequence and convert to ndarray\n+152\t            if isinstance(args[0], Sequence) and not arg0_is_arr:\n+153\t                args[0] = np.array(args[0])\n+154\t                arg0_is_arr = True\n+155\t\n+156\t            if all([arg0_is_arr, arg1_is_arr, arg2_is_arr]):\n+157\t                self._from_arrays(args[0], args[1], args[2], check_duplicates)\n+158\t            elif all([arg0_is_arr, arg1_is_arr]):\n \n-        elif len(args) == 3 or len(args) == 2:\n-            arg0_is_arr = isinstance(args[0], np.ndarray)\n-            arg1_is_arr = isinstance(args[1], np.ndarray)\n-            if len(args) == 3:\n-                arg2_is_arr = isinstance(args[2], np.ndarray)\n-            else:\n-                arg2_is_arr = False\n-\n-            if all([arg0_is_arr, arg1_is_arr, arg2_is_arr]):\n-                self._from_arrays(args[0], args[1], args[2], check_duplicates)\n-            elif all([arg0_is_arr, arg1_is_arr]):\n-                self._from_arrays(args[0], args[1], None, check_duplicates)\n-            else:\n-                raise TypeError(\"Arguments not understood by `RectilinearGrid`.\")\n \n     def __repr__(self):\n         \"\"\"Return the default representation.\"\"\"\n", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "pydicom__pydicom-1694", "model_patch": "diff --git a/pydicom/dataset.py b/pydicom/dataset.py\nindex e32315011..1444dd4e0 100644\n--- a/pydicom/dataset.py\n+++ b/pydicom/dataset.py\n@@ -492,17 +492,15 @@ class Dataset:\n         >>> 'SliceLocation' in ds\n         True\n \n-        Parameters\n-        ----------\n-        name : str or int or 2-tuple\n-            The element keyword or tag to search for.\n+        try:\n+            data_element = self[key]\n+        except Exception as exc:\n+            logger.error(f\"Error while accessing tag {json_key}: {exc}\")\n+            if suppress_invalid_tags:\n+                continue  # Skip this tag\n+            else:\n+                raise exc  # Raise the exception if suppression is not desired\n \n-        Returns\n-        -------\n-        bool\n-            ``True`` if the corresponding element is in the :class:`Dataset`,\n-            ``False`` otherwise.\n-        \"\"\"\n         try:\n             return Tag(name) in self._dict\n         except Exception as exc:\n", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "pydicom__pydicom-1413", "model_patch": "diff --git a/pydicom/dataset.py b/pydicom/dataset.py\nindex 323a50e88..3d95550de 100644\n--- a/pydicom/dataset.py\n+++ b/pydicom/dataset.py\n@@ -442,7 +442,6 @@ class Dataset:\n             Part 5, :dcm:`Section 6.2<part05/sect_6.2.html>`).\n         value\n             The value of the data element. One of the following:\n-\n             * a single string or number\n             * a :class:`list` or :class:`tuple` with all strings or all numbers\n             * a multi-value string with backslash separator\n@@ -450,10 +449,17 @@ class Dataset:\n               :class:`Dataset`\n         \"\"\"\n \n+        if VR == 'OL':\n+            if isinstance(value, list) or isinstance(value, tuple):\n+                value = array.array('H', value).tobytes()  # ensure it\u2019s converted to bytes\n+            if not isinstance(value, (bytes, bytearray)):\n+                raise TypeError(\"a bytes-like object is required for VR 'OL'\")\n+\n         data_element = DataElement(tag, VR, value)\n         # use data_element.tag since DataElement verified it\n         self._dict[data_element.tag] = data_element\n \n+\n     def __array__(self) -> \"numpy.ndarray\":\n         \"\"\"Support accessing the dataset from a numpy array.\"\"\"\n         return numpy.asarray(self._dict)\n", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "pydicom__pydicom-901", "model_patch": "diff --git a/pydicom/config.py b/pydicom/config.py\nindex ae9c8bdf1..f830e2cbb 100644\n--- a/pydicom/config.py\n+++ b/pydicom/config.py\n@@ -62,10 +62,6 @@ and datetime.time respectively. Default: False\n \n # Logging system and debug function to change logging level\n logger = logging.getLogger('pydicom')\n-handler = logging.StreamHandler()\n-formatter = logging.Formatter(\"%(message)s\")\n-handler.setFormatter(formatter)\n-logger.addHandler(handler)\n \n \n import pydicom.pixel_data_handlers.numpy_handler as np_handler  # noqa\n", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "pydicom__pydicom-1139", "model_patch": "diff --git a/pydicom/valuerep.py b/pydicom/valuerep.py\nindex e992f7872..d4821e04c 100644\n--- a/pydicom/valuerep.py\n+++ b/pydicom/valuerep.py\n@@ -826,6 +826,8 @@ class PersonName:\n                     (len(self._components) > 1 or bool(self._components[0])))\n         return bool(self.original_string)\n \n+    def __iter__(self):\n+        return iter(self.components)\n \n # Alias old class names for backwards compat in user code\n PersonNameUnicode = PersonName = PersonName\n", "model_name_or_path": "gpt-4o-mini"}, {"instance_id": "pydicom__pydicom-1256", "model_patch": "diff --git a/pydicom/dataset.py b/pydicom/dataset.py\nindex 5457dedfa..8b71e6304 100644\n--- a/pydicom/dataset.py\n+++ b/pydicom/dataset.py\n@@ -2267,7 +2267,9 @@ class Dataset(Dict[BaseTag, _DatasetValue]):\n             else:\n                 value_key = unique_value_keys[0]\n                 value = mapping[value_key]\n-            data_element = DataElement.from_json(\n+            if bulk_data_uri_handler:\n+                value = bulk_data_uri_handler(tag, vr, value)  # resolve via handler\n+\n                 cls, tag, vr, value, value_key, bulk_data_uri_handler\n             )\n             dataset.add(data_element)\n", "model_name_or_path": "gpt-4o-mini"}]